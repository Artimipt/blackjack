{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"program.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"code","metadata":{"id":"ebTCBKBQtxba"},"source":["from pydrive.auth import GoogleAuth\r\n","from pydrive.drive import GoogleDrive\r\n","from google.colab import auth\r\n","from oauth2client.client import GoogleCredentials"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mu0H0yf3oozv"},"source":["auth.authenticate_user()\r\n","gauth = GoogleAuth()\r\n","gauth.credentials = GoogleCredentials.get_application_default()\r\n","drive = GoogleDrive(gauth)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bI_HqUl4tqNO"},"source":["import sys\r\n","sys.path.append('/content/gdrive/MyDrive/проект/Сама работа/Blackjack-Reinforcement-Learning')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tORUc_T6-tmE","colab":{"base_uri":"https://localhost:8080/","height":367},"executionInfo":{"status":"error","timestamp":1608191453650,"user_tz":-180,"elapsed":941,"user":{"displayName":"Артур Рашидович Нигматзянов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgL5lVdkiW-Qc_AmgzF4nzD2DnS6MWgKZrYP9yg=s64","userId":"02512240593837269214"}},"outputId":"6ed8fbad-988f-4ca1-ade1-95247aaf4306"},"source":["from utils.state import State, states\n","from game.player import Player\n","from game.dealer import Dealer\n","from game.action import Action\n","from agents.monteCarloExploringStarts import MonteCarloExploringStarts\n","from agents.onPolicyMonteCarlo import OnPolicyMonteCarlo\n","from agents.qLearning import QLearning\n","from agents.deterministic import Deterministic\n","from agents.sarsa import Sarsa\n","from utils.tests import *\n","import sys\n","import time"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-e65d2f81feb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdealer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDealer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonteCarloExploringStarts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMonteCarloExploringStarts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"Gku1zhY4oNKA"},"source":["# Новый раздел"]},{"cell_type":"code","metadata":{"id":"fZ99eqpm-pEn","scrolled":true},"source":["algos = {\"deterministic\": Deterministic, \"monteCarloExploringStarts\": MonteCarloExploringStarts,\n","         \"onPolicyMonteCarlo\": OnPolicyMonteCarlo, \"qlearning\": QLearning, \"sarsa\": Sarsa}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WIu9V-_t-pEo","scrolled":true},"source":["deterministic_agent = Deterministic()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G0vuFZ73-pEp","scrolled":true},"source":["# Количество эпизодов\n","number = 2000000\n","\n","# Значение epsilon для алгоритмов\n","epsilon = 0.2\n","\n","# \n","alpha = 0.02\n","\n","# Улучшение результатов повторами\n","improve = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2D-Pff4f-pEp","scrolled":true},"source":["def Learning_and_test (name_algorithm, number, epsilon, alpha, improve):\n","    algo = algos[name_algorithm]\n","    agent = algo(epsilon, alpha, improve)\n","    start_time = time.time() # замеряем время выполнения\n","    policy = agent.calculate(number) #обучение, получаем соответствующую политику\n","    elapsed_time = print(\"Время выполнения: \", time.time() - start_time)\n","    print_differences(deterministic_agent.calculate(), policy, name_algorithm ) # отличия от детерминированного\n","    results = play_many_times(policy, times=2000000) # тестирование\n","    print(\"Количество побед = \", results[0])\n","    print(\"Количество ничей = \", results[1])\n","    print(\"Количество проигрышей = \", results[2])\n","    \n","    return results[2] # возвращаем процент поражений - эффективность алгоритма"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aeNjoTbb-pEq"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s9rbSk0G-pEq","scrolled":true},"source":["loss_deterministic =  Learning_and_test('deterministic', number, epsilon, alpha, improve)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0EzS6Kjs-pEr","scrolled":true},"source":["loss_qlearning =  Learning_and_test('qlearning', number, epsilon, alpha, improve)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kV8B7TvX-pEs","scrolled":true},"source":["loss_sarsa = Learning_and_test('sarsa', number, epsilon, alpha, improve)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tItBQGYT-pEs","scrolled":true},"source":["loss_monteCarloExploringStarts = Learning_and_test('monteCarloExploringStarts', number, epsilon, alpha, improve)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4edQimK-pEt","scrolled":true},"source":["loss_onPolicyMonteCarlo = Learning_and_test('onPolicyMonteCarlo', number, epsilon, alpha, improve)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K5o-HwQf-pEt"},"source":["# Effectivness of algorithms - losses:"]},{"cell_type":"code","metadata":{"id":"bcmbipWs-pEu"},"source":["print(f\"Deterministic_lose = {loss_deterministic}\", \n","      f\"QLearning_lose = {loss_qlearning}\", \n","      f\"Sarsa_lose = {loss_sarsa}\", \n","      f\"MonteCarloExploringStarts_lose = {loss_monteCarloExploringStarts}\",\n","      f\"OnPolicyMonteCarlo_lose = {loss_onPolicyMonteCarlo}\", sep = \"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P90cS5cG-pEu"},"source":[""],"execution_count":null,"outputs":[]}]}